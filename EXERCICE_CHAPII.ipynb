{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec112ce",
   "metadata": {},
   "source": [
    "### 1: Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a Lorem Ipsum of 3 paragraphs in a txt file using python, each paragraph delimited by two new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3589823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sit velit dolorem tempora etincidunt sed modi dolor. Dolore numquam numquam modi numquam est ut. Numquam neque modi porro numquam aliquam eius consectetur. Quiquia ut dolore porro dolore velit. Eius tempora dolore etincidunt aliquam labore. Amet ipsum modi neque dolor sit quaerat sit.\n",
      "\n",
      "\n",
      "Est tempora dolore sed dolorem voluptatem. Tempora adipisci dolorem modi tempora ipsum. Quisquam ut magnam est labore ut dolorem. Non sed dolorem quiquia numquam dolore velit tempora. Consectetur ut amet porro. Velit voluptatem dolore dolorem etincidunt porro. Dolorem magnam neque ut quisquam numquam labore.\n",
      "\n",
      "\n",
      "Quiquia non sed voluptatem dolore velit. Magnam dolore eius est ipsum numquam est. Amet ut etincidunt voluptatem sed. Dolore dolorem eius est. Neque etincidunt est consectetur dolorem magnam magnam numquam. Numquam ut ut quiquia ipsum porro. Dolorem tempora sit eius quisquam. Ut adipisci eius etincidunt est.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lorem\n",
    "\n",
    "def generate_lorem_ipsum_paragraphs(num_paragraphs=3):\n",
    "    # Cette fonction génère une liste de paragraphes Lorem Ipsum.\n",
    "    return [lorem.paragraph() for _ in range(num_paragraphs)]\n",
    "    \n",
    "def write_paragraphs_to_file(paragraphs, filename='lorem_ipsum.txt'):\n",
    "    # Cette fonction écrit les paragraphes dans un fichier, chaque paragraphe étant séparé par deux sauts de ligne.\n",
    "    with open(filename, 'w') as file:\n",
    "        for i, paragraph in enumerate(paragraphs):\n",
    "            file.write(paragraph)\n",
    "            if i < len(paragraphs) - 1:  # Ajoute des sauts de ligne seulement si ce n'est pas le dernier paragraphe\n",
    "                file.write('\\n\\n')\n",
    "\n",
    "paragraphs = generate_lorem_ipsum_paragraphs(3)\n",
    "write_paragraphs_to_file(paragraphs, 'lorem_ipsum.txt')  # Écrire dans un fichier\n",
    "\n",
    "# Pour affichage dans la console ou ici, affiche chaque paragraphe avec 2 sauts de ligne\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327215b1",
   "metadata": {},
   "source": [
    "### 2: Update the txt file by removing the first paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d340df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adipisci quaerat ut est labore dolor. Labore quisquam eius sit. Est quaerat consectetur velit aliquam non. Adipisci magnam quisquam ipsum. Etincidunt dolorem modi magnam eius.\n",
      "\n",
      "Magnam velit ut non. Amet eius quaerat dolorem ipsum. Magnam etincidunt numquam quisquam dolore magnam. Velit sed amet aliquam adipisci neque consectetur labore. Velit neque ipsum aliquam ipsum.\n",
      "/n\n"
     ]
    }
   ],
   "source": [
    "def remove_first_paragraph_and_update_file(filename='lorem_ipsum.txt'):\n",
    "\n",
    "    with open(filename, 'r') as file:\n",
    "        paragraphs = file.read().split('\\n\\n')\n",
    "    \n",
    "    if paragraphs:\n",
    "        paragraphs.pop(0)\n",
    "    \n",
    "    write_paragraphs_to_file(paragraphs, filename)\n",
    "\n",
    "paragraphs = generate_lorem_ipsum_paragraphs(3)\n",
    "write_paragraphs_to_file(paragraphs, 'lorem_ipsum.txt')\n",
    "\n",
    "remove_first_paragraph_and_update_file('lorem_ipsum.txt')\n",
    "\n",
    "with open('lorem_ipsum.txt', 'r') as file:\n",
    "    updated_content = file.read()\n",
    "    print(updated_content)\n",
    "    print(\"/n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a8380",
   "metadata": {},
   "source": [
    "### 3: Create a dict from the paper of lecun et al. and goodfellow et al. with authors, title, affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918e8cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article de LeCun et al. :\n",
      "{'authors': ['Yann LeCun', 'Corinna Cortes', 'Christopher J.C. Burges'], 'title': 'MNIST handwritten digit database', 'affiliations': ['Facebook AI Research (FAIR), New York, USA', 'Google Research, Mountain View, USA']}\n",
      "\n",
      "Article de Goodfellow et al. :\n",
      "{'authors': ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'title': 'Deep Learning', 'affiliations': ['Google Research, Mountain View, USA', 'Mila - Quebec AI Institute, Université de Montréal, Montreal, Canada']}\n"
     ]
    }
   ],
   "source": [
    "# Dictionnaire pour l'article de LeCun et al.\n",
    "lecun_paper = {\n",
    "    \"authors\": [\"Yann LeCun\", \"Corinna Cortes\", \"Christopher J.C. Burges\"],\n",
    "    \"title\": \"MNIST handwritten digit database\",\n",
    "    \"affiliations\": [\"Facebook AI Research (FAIR), New York, USA\", \"Google Research, Mountain View, USA\"]\n",
    "}\n",
    "\n",
    "# Dictionnaire pour l'article de Goodfellow et al.\n",
    "goodfellow_paper = {\n",
    "    \"authors\": [\"Ian Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"],\n",
    "    \"title\": \"Deep Learning\",\n",
    "    \"affiliations\": [\"Google Research, Mountain View, USA\", \"Mila - Quebec AI Institute, Université de Montréal, Montreal, Canada\"]\n",
    "}\n",
    "\n",
    "# Afficher les dictionnaires\n",
    "print(\"Article de LeCun et al. :\")\n",
    "print(lecun_paper)\n",
    "print(\"\\nArticle de Goodfellow et al. :\")\n",
    "print(goodfellow_paper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe2ad8",
   "metadata": {},
   "source": [
    "### 4: Save the previously created dict in the JSON format and load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d19d568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LeCun_paper': {'authors': ['Yann LeCun', 'Corinna Cortes', 'Christopher J.C. Burges'], 'title': 'MNIST handwritten digit database', 'affiliations': ['Facebook AI Research (FAIR), New York, USA', 'Google Research, Mountain View, USA']}, 'Goodfellow_paper': {'authors': ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'title': 'Deep Learning', 'affiliations': ['Google Research, Mountain View, USA', 'Mila - Quebec AI Institute, Université de Montréal, Montreal, Canada']}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Sauvegarder le dictionnaire dans un fichier JSON\n",
    "with open('articles.json', 'w') as f:\n",
    "    json.dump({\"LeCun_paper\": lecun_paper, \"Goodfellow_paper\": goodfellow_paper}, f)\n",
    "\n",
    "# Charger le dictionnaire depuis le fichier JSON\n",
    "with open('articles.json', 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "# Afficher les données chargées\n",
    "print(loaded_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f3a2e8",
   "metadata": {},
   "source": [
    "### 5: Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa37a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "€\u0004•Å\u0001\u0000\u0000\u0000\u0000\u0000\u0000}”(Œ\u000b",
      "LeCun_paper”}”(Œ\u0007authors”]”(Œ\n",
      "Yann LeCun”Œ\u000eCorinna Cortes”Œ\u0017Christopher J.C. Burges”eŒ\u0005title”Œ MNIST handwritten digit database”Œ\f",
      "affiliations”]”(Œ*Facebook AI Research (FAIR), New York, USA”Œ#Google Research, Mountain View, USA”euŒ\u0010Goodfellow_paper”}”(h\u0003]”(Œ\u000eIan Goodfellow”Œ\n",
      "Yoshua Bengio”Œ\u000fAaron Courville”eh\bŒ\n",
      "Deep Learning”h\n",
      "]”(Œ#Google Research, Mountain View, USA”ŒFMila - Quebec AI Institute, UniversitÃ© de MontrÃ©al, Montreal, Canada”euu.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('articles.pkl', 'wb') as file:\n",
    "    pickle.dump({\"LeCun_paper\": lecun_paper, \"Goodfellow_paper\": goodfellow_paper}, file)\n",
    "    \n",
    "with open('articles.pkl', 'r', errors='ignore') as file:\n",
    "    content = file.read()\n",
    "    \n",
    "print(content) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc9154",
   "metadata": {},
   "source": [
    "### 6: Parse the xml_file2 in the same way as in the lecture. put infos in a dict and save it in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2101019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in c:\\users\\oumis\\anaconda3\\oumconda\\lib\\site-packages (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xmltodict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135ade79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<note>\n",
      "  <date>2015-09-01</date>\n",
      "  <hour>08:30</hour>\n",
      "  <to>Tove</to>\n",
      "  <from>Jani</from>\n",
      "  <body>Don't forget me this weekend!</body>\n",
      "</note>\n",
      "\n",
      "{'note': {'date': '2015-09-01', 'hour': '08:30', 'to': 'Tove', 'from': 'Jani', 'body': \"Don't forget me this weekend!\"}}\n"
     ]
    }
   ],
   "source": [
    "import lxml.etree\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "xml_file = \"C:/Users/oumis/Downloads/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file)\n",
    "print(lxml.etree.tostring(root, encoding=\"unicode\", pretty_print=True))\n",
    "\n",
    "with open(xml_file, \"rb\") as file:\n",
    "    xml_dict = xmltodict.parse(file)\n",
    "    print(xml_dict)  \n",
    "\n",
    "json_data = json.dumps(xml_dict, indent=4)\n",
    "with open(\"xml_data.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abf819",
   "metadata": {},
   "source": [
    "### 7: Download an image of your choice and save it in either jpg or png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1dc6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "im = Image.open(requests.get(\"https://www.science-et-vie.com/wp-content/uploads/scienceetvie/2024/01/capture-6-750x410.png\", stream=True).raw)\n",
    "im.save(\"C:/Users/oumis/Desktop/Nosql/pingouin.png\", \"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb63eac",
   "metadata": {},
   "source": [
    "### 8: From the data/Chap2/data_world.json file, create a set of publisher type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a02051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of publisher types:\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSON file\n",
    "with open('C:/Users/oumis/Desktop/Nosql/data_world.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create an empty set to store publisher types\n",
    "publisher_types = set()\n",
    "\n",
    "# Extract publisher types from the data\n",
    "for entry in data:\n",
    "    if 'publisher_type' in entry:\n",
    "        publisher_types.add(entry['publisher_type'])\n",
    "\n",
    "# Print the set of publisher types\n",
    "print(\"Set of publisher types:\")\n",
    "for publisher_type in publisher_types:\n",
    "    print(publisher_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5849cb",
   "metadata": {},
   "source": [
    "### 9: From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2e45ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionnaire nettoyé enregistré dans data_world_cleaned.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('C:/Users/oumis/Desktop/Nosql/data_world.json', 'r') as f:\n",
    "    data_world = json.load(f)\n",
    "\n",
    "key_to_delete = 'key'\n",
    "if key_to_delete in data_world:\n",
    "    del data_world[key_to_delete]\n",
    "    print(\"Clé supprimée avec succès.\")\n",
    "\n",
    "with open('C:/Users/oumis/Desktop/Nosql/data_world_cleaned.json', 'w') as f:\n",
    "    json.dump(data_world, f, indent=4)\n",
    "\n",
    "print(\"Dictionnaire nettoyé enregistré dans data_world_cleaned.json.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4dcce5",
   "metadata": {},
   "source": [
    "### 10: From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a125ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence Matrix:\n",
      "\tR/P1D\tR/P1M\tR/P3M\tR/PT1S\tUnknown\tirregular\n",
      "public\t5\t3\t1\t1\t29\t4961\n",
      "Matrice de co-occurrence enregistrée dans co_occurrence_matrix.csv.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open('C:/Users/oumis/Desktop/Nosql/data_world.json', 'r') as f:\n",
    "    data_world = json.load(f)\n",
    "\n",
    "access_levels = [entry.get('accessLevel', 'Unknown') for entry in data_world]\n",
    "periodicities = [entry.get('accrualPeriodicity', 'Unknown') for entry in data_world]\n",
    "\n",
    "# Création de la matrice de co-occurrence\n",
    "unique_access_levels = np.unique(access_levels)\n",
    "unique_periodicities = np.unique(periodicities)\n",
    "co_occurrence_matrix = np.zeros((len(unique_access_levels), len(unique_periodicities)), dtype=int)\n",
    "\n",
    "for access_level, periodicity in zip(access_levels, periodicities):\n",
    "    row_idx = np.where(unique_access_levels == access_level)[0][0]\n",
    "    col_idx = np.where(unique_periodicities == periodicity)[0][0]\n",
    "    co_occurrence_matrix[row_idx, col_idx] += 1\n",
    "\n",
    "# Affichage de la matrice de co-occurrence\n",
    "print(\"Co-occurrence Matrix:\")\n",
    "print(\"\\t\" + \"\\t\".join(unique_periodicities))\n",
    "for i, access_level in enumerate(unique_access_levels):\n",
    "    print(access_level + \"\\t\" + \"\\t\".join(map(str, co_occurrence_matrix[i])))\n",
    "\n",
    "# Optionnel : Enregistrement de la matrice de co-occurrence dans un fichier CSV\n",
    "np.savetxt('co_occurrence_matrix.csv', co_occurrence_matrix, delimiter=',', fmt='%d', header=\"\\t\".join(unique_periodicities), comments='')\n",
    "\n",
    "print(\"Matrice de co-occurrence enregistrée dans co_occurrence_matrix.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
