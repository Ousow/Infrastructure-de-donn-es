{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab6f785",
   "metadata": {},
   "source": [
    "## CRUD operations\n",
    "### Create a new db name Todo and a new collection named \"CRUD_exercise\" and do the following:\n",
    "\n",
    "### 1: Take the dict created in the TODO 4 in chapter I and save it in the collection \"CRUD_exercise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8eb4aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\oumis\\anaconda3\\oumconda\\lib\\site-packages (4.6.2)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\oumis\\anaconda3\\oumconda\\lib\\site-packages (from pymongo) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357305a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b46001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('662d3ae2b4991738e6063ee9'), acknowledged=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionnaire pour l'article de LeCun et al.\n",
    "lecun_paper = {\n",
    "    \"authors\": [\"Yann LeCun\", \"Corinna Cortes\", \"Christopher J.C. Burges\"],\n",
    "    \"title\": \"MNIST handwritten digit database\",\n",
    "    \"affiliations\": [\"Facebook AI Research (FAIR), New York, USA\", \"Google Research, Mountain View, USA\"]\n",
    "}\n",
    "\n",
    "# Dictionnaire pour l'article de Goodfellow et al.\n",
    "goodfellow_paper = {\n",
    "    \"authors\": [\"Ian Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"],\n",
    "    \"title\": \"Deep Learning\",\n",
    "    \"affiliations\": [\"Google Research, Mountain View, USA\", \"Mila - Quebec AI Institute, Université de Montréal, Montreal, Canada\"]\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "# Sauvegarder le dictionnaire dans un fichier JSON\n",
    "with open('articles.json', 'w') as f:\n",
    "    json.dump({\"LeCun_paper\": lecun_paper, \"Goodfellow_paper\": goodfellow_paper}, f)\n",
    "\n",
    "# Charger le dictionnaire depuis le fichier JSON\n",
    "with open('articles.json', 'r') as f:\n",
    "    loaded_data = json.load(f)\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"Todo\"]\n",
    "collection = mydb[\"CRUD_exercise\"]\n",
    "collection.insert_one(loaded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9774bf",
   "metadata": {},
   "source": [
    "### 2: Insert 3 documents with key = x and values = 1, delete one of them. Which one is deleted first ? the most recent or oldest one ? increment the value of x to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5478d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 78.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "for i in tqdm.tqdm(range(3)):\n",
    "    post = {\"x\":1}\n",
    "    collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d064c93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 1, 'ok': 1.0}, acknowledged=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete one\n",
    "collection.delete_one({'x': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e9ee1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult({'n': 6, 'nModified': 6, 'ok': 1.0, 'updatedExisting': True}, acknowledged=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.update_many({'x': 1}, {'$inc': {'x': 3}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12d6ed",
   "metadata": {},
   "source": [
    "### 3: Insert the dict created in the TODO 6 Chapter I in the example collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb131c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"Todo\"]\n",
    "collection = mydb[\"CRUD_exercise\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7075e54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<note>\n",
      "  <date>2015-09-01</date>\n",
      "  <hour>08:30</hour>\n",
      "  <to>Tove</to>\n",
      "  <from>Jani</from>\n",
      "  <body>Don't forget me this weekend!</body>\n",
      "</note>\n",
      "\n",
      "{'note': {'date': '2015-09-01', 'hour': '08:30', 'to': 'Tove', 'from': 'Jani', 'body': \"Don't forget me this weekend!\"}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('662d3bbdb4991738e6063eef'), acknowledged=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lxml.etree\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "xml_file = \"C:/Users/oumis/Desktop/Nosql/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file)\n",
    "print(lxml.etree.tostring(root, encoding=\"unicode\", pretty_print=True))\n",
    "\n",
    "with open(xml_file, \"rb\") as file:\n",
    "    xml_dict = xmltodict.parse(file)\n",
    "    print(xml_dict)  \n",
    "\n",
    "json_data = json.dumps(xml_dict, indent=4)\n",
    "with open(\"xml_data.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client[\"Todo\"]\n",
    "collection = db[\"CRUD_exercise\"]\n",
    "collection.insert_one(xml_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9402d699",
   "metadata": {},
   "source": [
    "### 4: Get documents where authors key exist in the collection \"CRUD_exercise\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "285e15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "\n",
    "# Select database and collection\n",
    "db = client[\"Todo\"]\n",
    "collection = db[\"CRUD_exercise\"]\n",
    "\n",
    "# Query documents where the \"authors\" key exists\n",
    "query = {\"authors\": {\"$exists\": True}}\n",
    "result = collection.find(query)\n",
    "\n",
    "# Print the documents\n",
    "for doc in result:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e814a",
   "metadata": {},
   "source": [
    "### 5: Change the documents where x = 4 to x = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "931a12b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents updated: 6\n"
     ]
    }
   ],
   "source": [
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client[\"Todo\"]\n",
    "collection = db[\"CRUD_exercise\"]\n",
    "\n",
    "# Update documents where x = 4 to x = 1\n",
    "query = {\"x\": 4}\n",
    "update = {\"$set\": {\"x\": 1}}\n",
    "result = collection.update_many(query, update)\n",
    "\n",
    "# Print the number of documents updated\n",
    "print(\"Number of documents updated:\", result.modified_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db70b5",
   "metadata": {},
   "source": [
    "### 6: Find documents where author is not_mike and set author as real_mike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c336eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "doc = collection.find_one_and_update({'author': \"not_mike\"}, {'$set': {'author': \"real_mike\"}})\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "749bf73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertManyResult([ObjectId('662d3c18b4991738e6063ef5'), ObjectId('662d3c18b4991738e6063ef6')], acknowledged=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"Todo\"]\n",
    "collection = mydb[\"CRUD_exercise\"]\n",
    "\n",
    "# new_posts = list of dicts [{},{}]\n",
    "new_posts = [{\"author\": \"Mike\",\n",
    "              \"title\":\"Python is fun\",\n",
    "               \"text\": \"Another post!\",\n",
    "               \"tags\": [\"bulk\", \"insert\"],\n",
    "              # date object format (year,month,day,hour,minute)\n",
    "               \"date\": datetime.datetime(2009, 11, 12, 11, 14)},\n",
    "              {\"author\": \"Eliot\",\n",
    "               \"title\": \"MongoDB is fun\",\n",
    "               \"text\": \"and pretty easy too!\",\n",
    "               \"date\": datetime.datetime(2009, 11, 10, 10, 45)}]\n",
    "\n",
    "collection.insert_many(new_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33e01038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpdateResult({'n': 1, 'nModified': 1, 'ok': 1.0, 'updatedExisting': True}, acknowledged=True)\n"
     ]
    }
   ],
   "source": [
    "doc=collection.update_one({\"author\": {\"$ne\": \"mike\"}},\n",
    "                      {'$set': {\n",
    "                          'author': \"real_mike\"\n",
    "                          }\n",
    "                      }, upsert=False)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ba697",
   "metadata": {},
   "source": [
    "### 7: Delete documents where author is real_mike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6319d383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 1, 'ok': 1.0}, acknowledged=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.delete_one({'author': \"real_mike\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c755ce",
   "metadata": {},
   "source": [
    "## Managing DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8504a13",
   "metadata": {},
   "source": [
    "### 8: create a collection named \"CRUD_exercise_benchmark\" with 500k observations, ids increment of 2 (sequence:0,2,4,6,...1M). Give a random np.array with a key named \"values\" and use the insert_many. Then create an index on the id and benchmark queries before and after indexing. Did the index help ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "660d7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:01<00:00, 132576.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pymongo\n",
    "import tqdm\n",
    "\n",
    "\n",
    "list_of_insertion = []\n",
    "for i in tqdm.tqdm(range(1, 500001, 2)):\n",
    "    # Générer un tableau NumPy aléatoire de taille 10\n",
    "    values_array = np.random.rand(1)\n",
    "    \n",
    "    # Créer le document à insérer\n",
    "    post = {\n",
    "        \"user_id\": i,\n",
    "        \"values\": values_array.tolist()  # Convertir le tableau NumPy en liste pour l'insertion dans MongoDB\n",
    "    }\n",
    "    \n",
    "    # Ajouter le document à la liste des insertions\n",
    "    list_of_insertion.append(post)\n",
    "    \n",
    "    # Insérer les documents par lots de 15000\n",
    "    if i % 15000 == 0:\n",
    "        collection.insert_many(list_of_insertion)\n",
    "        list_of_insertion = []\n",
    "\n",
    "# Insérer les documents restants\n",
    "if list_of_insertion:\n",
    "    collection.insert_many(list_of_insertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c734d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionSuccess': True,\n",
       " 'nReturned': 0,\n",
       " 'executionTimeMillis': 0,\n",
       " 'totalKeysExamined': 0,\n",
       " 'totalDocsExamined': 0,\n",
       " 'executionStages': {'stage': 'FETCH',\n",
       "  'nReturned': 0,\n",
       "  'executionTimeMillisEstimate': 0,\n",
       "  'works': 1,\n",
       "  'advanced': 0,\n",
       "  'needTime': 0,\n",
       "  'needYield': 0,\n",
       "  'saveState': 0,\n",
       "  'restoreState': 0,\n",
       "  'isEOF': 1,\n",
       "  'docsExamined': 0,\n",
       "  'alreadyHasObj': 0,\n",
       "  'inputStage': {'stage': 'IXSCAN',\n",
       "   'nReturned': 0,\n",
       "   'executionTimeMillisEstimate': 0,\n",
       "   'works': 1,\n",
       "   'advanced': 0,\n",
       "   'needTime': 0,\n",
       "   'needYield': 0,\n",
       "   'saveState': 0,\n",
       "   'restoreState': 0,\n",
       "   'isEOF': 1,\n",
       "   'keyPattern': {'user_id': 1},\n",
       "   'indexName': 'user_id_1',\n",
       "   'isMultiKey': False,\n",
       "   'multiKeyPaths': {'user_id': []},\n",
       "   'isUnique': False,\n",
       "   'isSparse': False,\n",
       "   'isPartial': False,\n",
       "   'indexVersion': 2,\n",
       "   'direction': 'forward',\n",
       "   'indexBounds': {'user_id': ['[430, 430]']},\n",
       "   'keysExamined': 0,\n",
       "   'seeks': 1,\n",
       "   'dupsTested': 0,\n",
       "   'dupsDropped': 0}},\n",
       " 'allPlansExecution': []}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.find( { \"user_id\": 430 } ).explain()['executionStats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9fb2744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user_id_1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.create_index([ (\"user_id\",1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "320bd7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'executionSuccess': True,\n",
       " 'nReturned': 0,\n",
       " 'executionTimeMillis': 0,\n",
       " 'totalKeysExamined': 0,\n",
       " 'totalDocsExamined': 0,\n",
       " 'executionStages': {'stage': 'FETCH',\n",
       "  'nReturned': 0,\n",
       "  'executionTimeMillisEstimate': 0,\n",
       "  'works': 1,\n",
       "  'advanced': 0,\n",
       "  'needTime': 0,\n",
       "  'needYield': 0,\n",
       "  'saveState': 0,\n",
       "  'restoreState': 0,\n",
       "  'isEOF': 1,\n",
       "  'docsExamined': 0,\n",
       "  'alreadyHasObj': 0,\n",
       "  'inputStage': {'stage': 'IXSCAN',\n",
       "   'nReturned': 0,\n",
       "   'executionTimeMillisEstimate': 0,\n",
       "   'works': 1,\n",
       "   'advanced': 0,\n",
       "   'needTime': 0,\n",
       "   'needYield': 0,\n",
       "   'saveState': 0,\n",
       "   'restoreState': 0,\n",
       "   'isEOF': 1,\n",
       "   'keyPattern': {'user_id': 1},\n",
       "   'indexName': 'user_id_1',\n",
       "   'isMultiKey': False,\n",
       "   'multiKeyPaths': {'user_id': []},\n",
       "   'isUnique': False,\n",
       "   'isSparse': False,\n",
       "   'isPartial': False,\n",
       "   'indexVersion': 2,\n",
       "   'direction': 'forward',\n",
       "   'indexBounds': {'user_id': ['[430, 430]']},\n",
       "   'keysExamined': 0,\n",
       "   'seeks': 1,\n",
       "   'dupsTested': 0,\n",
       "   'dupsDropped': 0}},\n",
       " 'allPlansExecution': []}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.find( { \"user_id\": 430} ).explain()['executionStats']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457a332",
   "metadata": {},
   "source": [
    "#### Oui car il facilite le traitement et l'execution est plus vite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0b9e1",
   "metadata": {},
   "source": [
    "### 9: create a random collection in a random db and put the new collection in the tutorial DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f9c06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The collection 'qttazgrach' has been moved to the 'tutorial' database.\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Get a list of existing databases\n",
    "existing_databases = client.list_database_names()\n",
    "\n",
    "# Choose a random database name\n",
    "random_db_name = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
    "while random_db_name in existing_databases:\n",
    "    random_db_name = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
    "\n",
    "# Create a random database\n",
    "random_db = client[random_db_name]\n",
    "\n",
    "# Choose a random collection name\n",
    "random_collection_name = ''.join(random.choices(string.ascii_lowercase, k=10))\n",
    "\n",
    "# Generate some random data for the collection\n",
    "random_data = [{\"key\": ''.join(random.choices(string.ascii_lowercase, k=5)), \"value\": random.randint(1, 100)} for _ in range(10)]\n",
    "\n",
    "# Insert data into the random collection\n",
    "random_collection = random_db[random_collection_name]\n",
    "random_collection.insert_many(random_data)\n",
    "\n",
    "# Move the random collection to the \"tutorial\" database\n",
    "tutorial_db = client[\"tutorial\"]\n",
    "tutorial_db[random_collection_name].insert_many(random_collection.find())\n",
    "\n",
    "# Drop the random collection from the random database\n",
    "random_db.drop_collection(random_collection_name)\n",
    "\n",
    "# Print confirmation message\n",
    "print(f\"The collection '{random_collection_name}' has been moved to the 'tutorial' database.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8fead",
   "metadata": {},
   "source": [
    "### 10: What is the difference between an inner join and an outer join ? Is the query seen during course an inner or outer join ? Play with the query to show all the joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec3c0252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('662a3b95be3f945beb6bc012'), 'user_id': 1, 'user_name': 'William Freeman', 'cellmodels': {'user_id': 1, 'random_value': 100}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc014'), 'user_id': 3, 'user_name': 'David Fowler', 'cellmodels': {'user_id': 3, 'random_value': 300}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc016'), 'user_id': 5, 'user_name': 'Maria Baker', 'cellmodels': {'user_id': 5, 'random_value': 500}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc018'), 'user_id': 7, 'user_name': 'Danny Swafford', 'cellmodels': {'user_id': 7, 'random_value': 700}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc01a'), 'user_id': 9, 'user_name': 'Theresa Athayde', 'cellmodels': {'user_id': 9, 'random_value': 900}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc01c'), 'user_id': 11, 'user_name': 'Robert Head', 'cellmodels': {'user_id': 11, 'random_value': 1100}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc01e'), 'user_id': 13, 'user_name': 'Artie Masterson', 'cellmodels': {'user_id': 13, 'random_value': 1300}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc020'), 'user_id': 15, 'user_name': 'Jose Moffett', 'cellmodels': {'user_id': 15, 'random_value': 1500}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc022'), 'user_id': 17, 'user_name': 'Daria Miller', 'cellmodels': {'user_id': 17, 'random_value': 1700}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc024'), 'user_id': 19, 'user_name': 'Susan Pendergast', 'cellmodels': {'user_id': 19, 'random_value': 1900}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc026'), 'user_id': 21, 'user_name': 'Donald Weinzierl', 'cellmodels': {'user_id': 21, 'random_value': 2100}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc028'), 'user_id': 23, 'user_name': 'Katie Allen', 'cellmodels': {'user_id': 23, 'random_value': 2300}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc02a'), 'user_id': 25, 'user_name': 'Beverly Green', 'cellmodels': {'user_id': 25, 'random_value': 2500}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc02c'), 'user_id': 27, 'user_name': 'Michael Mcquay', 'cellmodels': {'user_id': 27, 'random_value': 2700}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc02e'), 'user_id': 29, 'user_name': 'Billy Godwin', 'cellmodels': {'user_id': 29, 'random_value': 2900}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc030'), 'user_id': 31, 'user_name': 'Glenn Adams', 'cellmodels': {'user_id': 31, 'random_value': 3100}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc032'), 'user_id': 33, 'user_name': 'Jason Johnson', 'cellmodels': {'user_id': 33, 'random_value': 3300}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc034'), 'user_id': 35, 'user_name': 'David Chandler', 'cellmodels': {'user_id': 35, 'random_value': 3500}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc036'), 'user_id': 37, 'user_name': 'Norma Svoboda', 'cellmodels': {'user_id': 37, 'random_value': 3700}}\n",
      "{'_id': ObjectId('662a3b95be3f945beb6bc038'), 'user_id': 39, 'user_name': 'Zachary Breland', 'cellmodels': {'user_id': 39, 'random_value': 3900}}\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"benchmark\"]\n",
    "\n",
    "pipeline = [{'$lookup': \n",
    "                {'from' : 'benchmark_2',\n",
    "                 'localField' : 'user_id',\n",
    "                 'foreignField' : 'user_id',\n",
    "                 'as' : 'cellmodels'}},\n",
    "            {'$unwind': '$cellmodels'},\n",
    "            {'$project': \n",
    "                {'user_id':1,\"user_name\":1, 'cellmodels.user_id':1, 'cellmodels.random_value':1}} \n",
    "             ]\n",
    "\n",
    "documents = collection.aggregate(pipeline)\n",
    "for i in range(20):\n",
    "    print(next(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e8111",
   "metadata": {},
   "source": [
    "## Real world problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ed8f2",
   "metadata": {},
   "source": [
    "### 11: Use the oaipmh and api code get papers after January 2020 and for \"cs,math,econ\" categories. Insert them in MongoDB. Import only the first 200. How is it sorted ? How can you define your own sort()? Query papers to get papers after 2021, which have 3 authors and with domain \"cs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1a1568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Obtaining dependency information for feedparser from https://files.pythonhosted.org/packages/7c/d4/8c31aad9cc18f451c49f7f9cfb5799dadffc88177f7917bc90a66459b1d7/feedparser-6.0.11-py3-none-any.whl.metadata\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 30.7/81.3 kB 660.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 81.3/81.3 kB 1.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6061 sha256=1ed33ee139448a18d92ae7172673e0781151bc39cf001c2afbc97e30f489e4a0\n",
      "  Stored in directory: c:\\users\\oumis\\appdata\\local\\pip\\cache\\wheels\\3b\\25\\2a\\105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31a3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 72100/617968 [31:16<3:34:23, 42.43it/s] "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import feedparser\n",
    "import tqdm\n",
    "import time\n",
    "import pymongo\n",
    "\n",
    "# For each id get all the metadata https://info.arxiv.org/help/api/basics.html#python_simple_example\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"arxiv_api\"]\n",
    "\n",
    "# get list of ids previously downloaded\n",
    "with open(\"C:/Users/oumis/Desktop/Nosql/arxiv_cs.txt\",\"r\") as lines:\n",
    "    ids = list(set(lines.read().split(\"\\n\")[0:-2]))\n",
    "\n",
    "#init list of ids and iteration\n",
    "ids_query = []\n",
    "\n",
    "# loop through ids\n",
    "for id_ in tqdm.tqdm(ids):\n",
    "    #append id to list\n",
    "    ids_query.append(id_)\n",
    "    # if len list = 100 \n",
    "    if len(ids_query) == 100 :\n",
    "        # collapse list of id\n",
    "        ids_query = \",\".join(ids_query)\n",
    "        # query the api for the 100 ids\n",
    "        response = requests.get('http://export.arxiv.org/api/query?id_list={}&max_results=100'.format(ids_query))\n",
    "        # parse response\n",
    "        feed = feedparser.parse(response.content)\n",
    "        # commit the 100 papers found\n",
    "        list_of_insertion = []\n",
    "        for entry in feed.entries:\n",
    "            list_of_insertion.append(dict(entry))\n",
    "        collection.insert_many(list_of_insertion)\n",
    "        ids_query = []\n",
    "        time.sleep(1/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae8725",
   "metadata": {},
   "source": [
    "### 12: Do the same as exercise 8 but with the connection to the cluster. Then check the metrics and take screenshot of opcounters, logical size and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0d94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opcounters: {'insert': 11255077, 'query': 449, 'update': 100265, 'delete': 11, 'getmore': 1, 'command': 140991}\n",
      "Logical Size: 59002157.0\n",
      "Connections: {'current': 26, 'available': 999974, 'totalCreated': 186, 'rejected': 0, 'active': 8, 'threaded': 26, 'exhaustIsMaster': 0, 'exhaustHello': 6, 'awaitingTopologyChanges': 7}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to the MongoDB cluster\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "# Access the desired database and collection\n",
    "db = client[\"Todo\"]\n",
    "collection = db[\"CRUD_exercise\"]\n",
    "\n",
    "# Fetch the metrics\n",
    "opcounters = db.command(\"serverStatus\")[\"opcounters\"]\n",
    "logical_size = db.command(\"dbstats\")[\"dataSize\"]\n",
    "connections = db.command(\"serverStatus\")[\"connections\"]\n",
    "\n",
    "# Print the metrics (you can also save them or take screenshots)\n",
    "print(\"Opcounters:\", opcounters)\n",
    "print(\"Logical Size:\", logical_size)\n",
    "print(\"Connections:\", connections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4fbf0",
   "metadata": {},
   "source": [
    "### 13: Download a random image and store it in a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c48b5c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image stored in MongoDB successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pymongo import MongoClient\n",
    "from bson.binary import Binary\n",
    "\n",
    "# Function to download and store image in MongoDB\n",
    "def download_and_store_image(url):\n",
    "    # Download the image from the URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        image_data = response.content\n",
    "\n",
    "        # Connect to MongoDB\n",
    "        client = MongoClient('localhost', 27017)\n",
    "        db = client[\"Todo\"]\n",
    "        collection = db[\"CRUD_exercise\"]\n",
    "\n",
    "        # Store the image data in MongoDB\n",
    "        document = {\"image\": Binary(image_data)}\n",
    "        collection.insert_one(document)\n",
    "        print(\"Image stored in MongoDB successfully.\")\n",
    "    else:\n",
    "        print(\"Failed to download the image.\")\n",
    "\n",
    "# Example usage: provide a URL of the image you want to download and store\n",
    "image_url = \"https://next.ink/wp-content/uploads/2023/12/netflix-nouveau-logo-1536x864.jpg\"\n",
    "download_and_store_image(image_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6424e2",
   "metadata": {},
   "source": [
    "### 14: Try to store a pandas dataframe in mongoDB (array with rownames, array with colnames and matrix with values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b5f8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame stored in MongoDB successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert DataFrame to dictionary\n",
    "df_dict = {\n",
    "    'row_names': df.index.tolist(),\n",
    "    'col_names': df.columns.tolist(),\n",
    "    'values': df.values.tolist()\n",
    "}\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client[\"Todo\"]\n",
    "collection = db[\"DataFrame\"]\n",
    "\n",
    "# Insert the dictionary into MongoDB\n",
    "collection.insert_one(df_dict)\n",
    "print(\"DataFrame stored in MongoDB successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8503517",
   "metadata": {},
   "source": [
    "### 15: Insert the movie_review.tsv data into mongodb. Then query it to find the number of review that are positive and negative review. Fetch the docs which have \"unexpected\" in their review, how many are they ? Think of a clever way to count the number of words in the review using MongoDB (hint: Transform the review text before the insert in MongoDB) and create a density of number of words per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc7464c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted into MongoDB successfully.\n",
      "Number of positive reviews: 0\n",
      "Number of negative reviews: 0\n",
      "Number of reviews containing 'unexpected': 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "\n",
    "# Read the TSV file and transform it into a list of dictionaries\n",
    "df = pd.read_csv('C:/Users/oumis/Downloads/movie_review.tsv', sep='\\t')\n",
    "reviews = df.to_dict(orient='records')\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client[\"Todo\"]\n",
    "collection = db[\"movie_reviews\"]\n",
    "\n",
    "# Insert the list of dictionaries into MongoDB\n",
    "collection.insert_many(reviews, ordered=False)\n",
    "print(\"Data inserted into MongoDB successfully.\")\n",
    "\n",
    "# Query the collection to find the number of positive and negative reviews\n",
    "positive_reviews_count = collection.count_documents({\"sentiment\": \"positive\"})\n",
    "negative_reviews_count = collection.count_documents({\"sentiment\": \"negative\"})\n",
    "print(\"Number of positive reviews:\", positive_reviews_count)\n",
    "print(\"Number of negative reviews:\", negative_reviews_count)\n",
    "\n",
    "\n",
    "# Fetch the documents that contain \"unexpected\" in their review\n",
    "unexpected_reviews = collection.find({\"review\": {\"$regex\": \"unexpected\"}}, {\"_id\": 0, \"review\": 1})\n",
    "unexpected_reviews_count = 0\n",
    "for _ in unexpected_reviews:\n",
    "    unexpected_reviews_count += 1\n",
    "\n",
    "print(\"Number of reviews containing 'unexpected':\", unexpected_reviews_count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541191cf",
   "metadata": {},
   "source": [
    "### 16: Download a sound sample. Try to store it in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "882a1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound sample stored in MongoDB successfully.\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import gridfs\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['sound_samples']\n",
    "fs = gridfs.GridFS(db)\n",
    "\n",
    "# Function to store sound sample in MongoDB\n",
    "def store_sound_sample(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = file.read()\n",
    "        fs.put(data, filename=file_path.split('/')[-1])  # Store the file in GridFS\n",
    "        print(\"Sound sample stored in MongoDB successfully.\")\n",
    "\n",
    "# Example usage: provide the file path of the sound sample you want to store\n",
    "file_path = \"C:/Users/oumis/Downloads/sound2mongo.wav\"\n",
    "store_sound_sample(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4041830",
   "metadata": {},
   "source": [
    "### 17: Create a collection with 30M observation with a single key : \"year\" which is a random value between 2000-2020. Get documents with year = 2000. Does using an index helps ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16672df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client['test_db']\n",
    "collection = db['test_collection']\n",
    "\n",
    "# Function to generate random \"year\" values between 2000 and 2020\n",
    "def generate_random_year():\n",
    "    return random.randint(2000, 2020)\n",
    "\n",
    "# Insert 30M documents with random \"year\" values\n",
    "start_time = time.time()\n",
    "for _ in range(30000000):\n",
    "    document = {\"year\": generate_random_year()}\n",
    "    collection.insert_one(document)\n",
    "end_time = time.time()\n",
    "print(\"Time taken to insert 30M documents:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "# Query documents with \"year\" equal to 2000 without an index\n",
    "start_time = time.time()\n",
    "query_result = collection.find({\"year\": 2000})\n",
    "end_time = time.time()\n",
    "print(\"Time taken to query documents without an index:\", end_time - start_time, \"seconds\")\n",
    "print(\"Number of documents with year = 2000:\", query_result.count())\n",
    "\n",
    "# Create an index on the \"year\" field\n",
    "collection.create_index([(\"year\", pymongo.ASCENDING)])\n",
    "\n",
    "# Query documents with \"year\" equal to 2000 with an index\n",
    "start_time = time.time()\n",
    "query_result = collection.find({\"year\": 2000})\n",
    "end_time = time.time()\n",
    "print(\"Time taken to query documents with an index:\", end_time - start_time, \"seconds\")\n",
    "print(\"Number of documents with year = 2000:\", query_result.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d9388a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
